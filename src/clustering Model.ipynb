{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import data\n",
    "import utils\n",
    "import models\n",
    "from domain import get_domain\n",
    "from engines.engine import Criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--selection_model_file'], dest='selection_model_file', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='path to save the selection model', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='testing script')\n",
    "parser.add_argument('--data', type=str, default='data/negotiate', help='location of the data corpus')\n",
    "parser.add_argument('--unk_threshold', type=int, default=20, help='minimum word frequency to be in dictionary')\n",
    "parser.add_argument('--model_file', type=str, help='pretrained model file')\n",
    "parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
    "parser.add_argument('--hierarchical', action='store_true', default=False, help='use hierarchical model')\n",
    "parser.add_argument('--bsz', type=int, default=16, help='batch size')\n",
    "parser.add_argument('--cuda', action='store_true', default=False, help='use CUDA')\n",
    "parser.add_argument('--domain', type=str, default='object_division', help='domain for the dialogue')\n",
    "parser.add_argument('--sep_sel', action='store_true', default=False, help='use separate classifiers for selection')\n",
    "parser.add_argument('--model_type', type=str, default='rnn_model', help='model type', choices=models.get_model_names())\n",
    "parser.add_argument('--lr', type=float, default=20.0, help='initial learning rate')\n",
    "parser.add_argument('--min_lr', type=float, default=1e-5, help='min threshold for learning rate annealing')\n",
    "parser.add_argument('--decay_rate', type=float,  default=9.0, help='decrease learning rate by this factor')\n",
    "parser.add_argument('--decay_every', type=int,  default=1, help='decrease learning rate after decay_every epochs')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, help='momentum for sgd')\n",
    "parser.add_argument('--clip', type=float, default=0.2, help='gradient clipping')\n",
    "parser.add_argument('--visual', action='store_true', default=False, help='plot graphs')\n",
    "parser.add_argument('--sample_file', type=str, help='pretrained model file')\n",
    "parser.add_argument('--prediction_model_file', type=str,  default='', help='path to save the prediction model')\n",
    "parser.add_argument('--selection_model_file', type=str,  default='', help='path to save the selection model')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--cuda', '--model_file=clustering_model.th', '--bsz=1', '--domain=object_division', '--sep_sel', '--model_type=latent_clustering_model', '--lr=0.001', '--selection_model_file=selection_model.th'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset data/negotiate/train.txt, total 687919, unks 8718, ratio 1.27%\n",
      "sample input i2w:  ['2', '1', '1', '6', '2', '1']\n",
      "sample words:  ['YOU:', \"i'd\", 'like', 'the', 'hat', 'and', '1', 'ball', '.', '<eos>', 'THEM:', 'i', 'need', 'both', 'balls', '<eos>', 'YOU:', 'ok', '.', '<eos>', 'THEM:', '<selection>']\n",
      "Sample output:  ['item0=2', 'item1=1', 'item2=0', 'item0=0', 'item1=0', 'item2=2']\n",
      "dataset data/negotiate/val.txt, total 74653, unks 914, ratio 1.22%\n",
      "sample input i2w:  ['1', '10', '3', '0', '1', '0']\n",
      "sample words:  ['YOU:', 'i', 'need', 'the', 'book', 'and', 'two', 'hats', '<eos>', 'THEM:', 'i', 'get', 'the', 'ball', 'and', '1', 'hat', '<eos>', 'YOU:', 'actually', 'i', 'just', 'need', 'the', 'book', ',', 'so', 'you', 'can', 'have', 'the', 'rest', 'of', 'it', '<eos>', 'THEM:', 'you', 'get', 'book', 'since', 'its', 'worth', '10', 'to', 'you', 'i', 'get', 'the', 'rest', '.', 'deal', '<eos>', 'YOU:', '<selection>']\n",
      "Sample output:  ['item0=1', 'item1=0', 'item2=0', 'item0=0', 'item1=3', 'item2=1']\n",
      "dataset data/negotiate/test.txt, total 70262, unks 847, ratio 1.21%\n",
      "sample input i2w:  ['2', '4', '1', '2', '2', '0']\n",
      "sample words:  ['THEM:', 'i', 'want', 'the', 'balls', 'and', 'a', 'book', '<eos>', 'YOU:', 'i', 'can', 'not', 'make', 'that', 'deal', '.', 'you', 'can', 'have', 'the', 'balls', 'but', ',', 'i', 'need', 'the', 'books', 'and', 'the', 'hat', '<eos>', 'THEM:', 'ok', 'deal', '<eos>', 'YOU:', 'okay', 'great', '.', 'thank', 'you', '!', '<eos>', 'THEM:', '<selection>']\n",
      "Sample output:  ['item0=2', 'item1=1', 'item2=0', 'item0=0', 'item1=0', 'item2=2']\n",
      "| valid | avg entropy 0.090 | avg max prob 0.802 | avg top3 prob 0.835 \n",
      "| valid | enc avg entropy 0.229 | enc avg max prob 0.759 | enc avg top3 prob 0.824 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_69686/3501704843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#test_loss /= testset_stats['nonpadn']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#test_select_loss /= len(testset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testloss %.3f | testppl %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testselectloss %.3f | testselectppl %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_select_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_select_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "device_id = utils.use_cuda(args.cuda)\n",
    "utils.set_seed(args.seed)\n",
    "\n",
    "domain = get_domain(args.domain)\n",
    "model_ty = models.get_model_type(args.model_type)\n",
    "\n",
    "corpus = model_ty.corpus_ty(domain, args.data, freq_cutoff=args.unk_threshold,\n",
    "        verbose=True, sep_sel=args.sep_sel)\n",
    "\n",
    "model = utils.load_model(args.model_file)\n",
    "\n",
    "crit = Criterion(model.word_dict, device_id=device_id)\n",
    "sel_crit = Criterion(model.item_dict, device_id=device_id,\n",
    "        bad_toks=['<disconnect>', '<disagree>'])\n",
    "\n",
    "engine = model_ty.engine_ty(model, args, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "testset, testset_stats = corpus.test_dataset(args.bsz)\n",
    "\n",
    "total_valid_loss, total_select_loss, total_partner_ctx_loss, extra = engine.test_pass(testset, testset_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _forward(self, batch):\n",
    "        ctx, _, inpts, lens, tgts, sel_tgt, rev_idxs, hid_idxs, cnt = batch\n",
    "        ctx = Variable(ctx)\n",
    "        cnt = Variable(cnt)\n",
    "        inpts = [Variable(inpt) for inpt in inpts]\n",
    "        tgts = [Variable(tgt) for tgt in tgts]\n",
    "        rev_idxs = [Variable(idx) for idx in rev_idxs]\n",
    "        hid_idxs = [Variable(idx) for idx in hid_idxs]\n",
    "\n",
    "        losses, stats = self.model.forward(inpts, tgts, hid_idxs, ctx, cnt)\n",
    "\n",
    "        return losses, stats, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra['output'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_map(batch, sep_sel=True):\n",
    "    ctx, _, inpts, lens, _, sel_tgt, rev_idxs, hid_idxs, _ = batch\n",
    "    ctx = Variable(ctx)\n",
    "    inpts = [Variable(inpt) for inpt in inpts]\n",
    "    rev_idxs = [Variable(idx) for idx in rev_idxs]\n",
    "    hid_idxs = [Variable(idx) for idx in hid_idxs]\n",
    "    if sep_sel:\n",
    "        sel_tgt = Variable(sel_tgt)\n",
    "    else:\n",
    "        sel_tgt = [Variable(t) for t in sel_tgt]\n",
    "\n",
    "        # remove YOU:/THEM: from the end\n",
    "    return(inpts[:-1], lens[:-1], rev_idxs[:-1], hid_idxs[:-1], ctx, sel_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "target = []\n",
    "output = []\n",
    "for i in range(len(extra['input'])):\n",
    "    in1, in2, in3, in4, in5, tgt = input_map(extra['input'][i])\n",
    "    out1 = extra['output'][i] \n",
    "    dialog = ''\n",
    "    for i in in1:\n",
    "        dialog = dialog + ' '.join(corpus.word_dict.i2w(i)) + ' '\n",
    "    sentences.append(dialog)\n",
    "    target.append(corpus.item_dict_old.i2w(tgt))\n",
    "    output.append(corpus.item_dict_old.i2w(torch.argmax(out1, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'sentences': sentences, 'targets':target, 'outputs':output})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['targets'] == df['outputs']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in in1:\n",
    "    print(corpus.word_dict.i2w(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.word_dict.i2w(in1[0][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.context_dict.i2w(in5[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.word_dict.i2w(in1[0][:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.item_dict_old.i2w(in5[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             batches.append((ctx, partner_ctx, inpts, lens, tgts, sel_tgt, rev_idxs, hid_idxs, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_readable(corpus, batch):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
